# observer/reflection_trainer.py
import logging
import json
import os
from datetime import datetime
import random # For placeholder interaction
from typing import Optional, Dict, List

# Assuming config access is handled
try:
    from core.config import get_config, ROOT_DIR
except ImportError:
    current_dir = os.path.dirname(os.path.abspath(__file__))
    ROOT_DIR = os.path.dirname(current_dir)
    print(f"Warning: Could not import from core.config. Using ROOT_DIR: {ROOT_DIR}")
    def get_config(key, default=None):
        if key == "v2_4.enable_guided_feedback":
            return True
        if key == "paths.logs":
            return os.path.join(ROOT_DIR, "logs")
        return default

logger = logging.getLogger(__name__)

class ReflectionTrainer:
    """Handles guided reflection with the player after matches to gather supervised feedback."""

    def __init__(self, learning_coach_instance=None):
        """
        Initializes the Reflection Trainer.
        Args:
            learning_coach_instance: An instance of LearningCoach to update profiles with feedback.
        """
        self.enabled = get_config("v2_4.enable_guided_feedback", True)
        self.learning_coach = learning_coach_instance # Needs the LearningCoach to save feedback
        self.feedback_log_path = os.path.join(get_config("paths.logs", os.path.join(ROOT_DIR, "logs")), "reflection_feedback.jsonl") # Log feedback separately
        
        if self.enabled:
            logger.info("Reflection Trainer initialized.")
            if not self.learning_coach:
                 logger.warning("LearningCoach instance not provided to ReflectionTrainer. Feedback won't update player profiles directly.")
        else:
            logger.info("Reflection Trainer is disabled in config.yaml.")

    def can_start_reflection(self) -> bool:
        """Checks if guided reflection is enabled."""
        return self.enabled

    def start_reflection_session(self, match_report: dict, summoner_name: str) -> Optional[str]:
        """
        Starts a guided reflection session based on a match report.
        Selects a key moment or pattern from the report to ask the player about.

        Args:
            match_report (dict): The report generated by GameAnalyzer.
            summoner_name (str): The player's name.

        Returns:
            str: The question posed to the player, or None if no suitable question found or disabled.
        """
        if not self.enabled or not match_report:
            return None

        logger.info(f"Starting reflection session for {summoner_name} based on match {match_report.get('match_id')}")

        # --- Logic to select a reflection point --- 
        question = None
        reflection_point = None

        # Prioritize asking about major failures or missed opportunities
        failures = match_report.get("tactical_analysis", {}).get("major_failures", [])
        missed_opportunities = match_report.get("coaching_summary", {}).get("missed_opportunities", [])

        potential_points = failures + missed_opportunities
        
        if potential_points:
            # Select a random point for simplicity (could be more sophisticated)
            chosen_point = random.choice(potential_points)
            # Formulate a question (needs refinement)
            if chosen_point in failures:
                 question = f"Durante a partida, identifiquei um momento crítico: 	'{chosen_point}	'. Você concorda que isso foi um erro ou houve outro fator? (sim/não/talvez)"
                 reflection_point = {"type": "failure_agreement", "detail": chosen_point}
            elif chosen_point in missed_opportunities:
                 question = f"Sugeri 	'{chosen_point}	', mas a sugestão foi ignorada. Você acha que seguir a sugestão teria sido melhor? (sim/não/talvez)"
                 reflection_point = {"type": "ignored_suggestion_review", "detail": chosen_point}
        else:
            # Ask about a general aspect if no specific points
            # Example: Ask about a specific success
            successes = match_report.get("tactical_analysis", {}).get("major_successes", [])
            if successes:
                 chosen_success = random.choice(successes)
                 question = f"Um ponto positivo foi: 	'{chosen_success}	'. Você sentiu que essa jogada foi bem executada? (sim/não)"
                 reflection_point = {"type": "success_confirmation", "detail": chosen_success}
            else:
                 logger.info("No specific reflection points found in the report.")
                 return None # Or ask a generic question

        if question:
            self.current_reflection = {"question": question, "context": reflection_point, "match_id": match_report.get("match_id"), "summoner_name": summoner_name}
            logger.debug(f"Generated reflection question: {question}")
            return question
        else:
            logger.warning("Could not generate a reflection question.")
            return None

    def record_feedback(self, player_response: str) -> None:
        """
        Records the player's feedback to the posed question.

        Args:
            player_response (str): The player's answer (e.g., "sim", "não", "talvez", or free text).
        """
        if not self.enabled or not hasattr(self, 'current_reflection') or not self.current_reflection:
            logger.warning("Cannot record feedback: No active reflection session.")
            return

        feedback_data = {
            "timestamp": datetime.now().isoformat(),
            "summoner_name": self.current_reflection["summoner_name"],
            "match_id": self.current_reflection["match_id"],
            "question_asked": self.current_reflection["question"],
            "reflection_context": self.current_reflection["context"],
            "player_response": player_response.lower().strip()
        }

        # Log the feedback to a file
        try:
            with open(self.feedback_log_path, 'a', encoding='utf-8') as f:
                f.write(json.dumps(feedback_data, ensure_ascii=False) + '\n')
            logger.info(f"Recorded reflection feedback for {feedback_data['summoner_name']}")
        except Exception as e:
            logger.exception(f"Failed to log reflection feedback: {e}")

        # --- Update Learning Coach Profile (if available) --- 
        if self.learning_coach:
            try:
                # This requires the LearningCoach to have a method to process this feedback
                # e.g., self.learning_coach.update_profile_with_reflection(feedback_data)
                logger.debug("Placeholder: Would update LearningCoach profile here.")
                # Example logic: If player disagrees with a failure, reduce confidence in that pattern
                # if feedback_data["reflection_context"]["type"] == "failure_agreement" and feedback_data["player_response"] == "não":
                #     self.learning_coach.adjust_pattern_confidence(feedback_data["reflection_context"]["detail"], adjustment=-0.1)
            except AttributeError:
                 logger.error("LearningCoach instance does not have the required method to update profile with reflection.")
            except Exception as e:
                 logger.exception(f"Error updating LearningCoach profile with reflection feedback: {e}")

        # Clear the current reflection session
        self.current_reflection = None

# Example Usage
# if __name__ == "__main__":
#     logging.basicConfig(level=logging.DEBUG, format=\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')
#     trainer = ReflectionTrainer()
#
#     mock_report = {
#         "match_id": "NA1_11223344",
#         "tactical_analysis": {
#             "major_failures": ["Fought outnumbered near Baron"],
#             "major_successes": ["Good flank in bot lane fight"]
#         },
#         "coaching_summary": {
#             "missed_opportunities": ["Ignored call to reset before dragon"]
#         }
#     }
#
#     if trainer.can_start_reflection():
#         question = trainer.start_reflection_session(mock_report, "ReflectPlayer")
#         if question:
#             print(f"\nQuestion for player: {question}")
#             # Simulate player response
#             response = random.choice(["sim", "não", "talvez"])
#             print(f"Player response: {response}")
#             trainer.record_feedback(response)
#
#             # Try another reflection on the same report (might ask different question)
#             question2 = trainer.start_reflection_session(mock_report, "ReflectPlayer")
#             if question2:
#                  print(f"\nSecond question: {question2}")
#                  response2 = "sim"
#                  print(f"Player response: {response2}")
#                  trainer.record_feedback(response2)
#             else:
#                  print("\nNo further reflection points found for this session.")
#         else:
#             print("\nCould not generate reflection question for this report.")
#     else:
#         print("Reflection Trainer is disabled.")

